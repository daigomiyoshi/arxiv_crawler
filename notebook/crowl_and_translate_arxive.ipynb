{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for Google colab\n",
    "# !pip install feedparser\n",
    "# !pip install googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import feedparser\n",
    "from urllib.parse import urlencode\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_url = 'http://export.arxiv.org/api/'\n",
    "\n",
    "# keywords = ['cat: stat.ML', 'cat: cs.AI']\n",
    "keywords = ['cat: stat.ML']\n",
    "start=0\n",
    "max_results = 500\n",
    "sort_by='submittedDate'\n",
    "sort_order=\"descending\"\n",
    "\n",
    "days = 20\n",
    "prune = True\n",
    "debug = False\n",
    "\n",
    "# paper_num_result = []\n",
    "# dict_key_keyword = 'keyword'\n",
    "# dict_key_n_of_papers = 'n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crowl arxiv articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_list(root_url, keyword, prune, start, max_results, sort_by, sort_order, days, debug):\n",
    "    results = query(\n",
    "        root_url = root_url,\n",
    "        search_query=keyword,\n",
    "        prune=prune,\n",
    "        start=start,\n",
    "        max_results=max_results,\n",
    "        sort_by=sort_by,\n",
    "        sort_order=sort_order\n",
    "    )\n",
    "    return select_recent_papers(from_papers_list=results, days=days, debug=debug)\n",
    "\n",
    "\n",
    "def query(root_url, search_query, prune, start, max_results, sort_by, sort_order):\n",
    "    url_args = urlencode({\"search_query\": search_query,\n",
    "                                          \"start\": start,\n",
    "                                          \"max_results\": max_results,\n",
    "                                          \"sortBy\": sort_by,\n",
    "                                          \"sortOrder\": sort_order})\n",
    "    results = feedparser.parse(root_url + 'query?' + url_args)\n",
    "    if results.get('status') != 200:\n",
    "        raise Exception(\n",
    "            \"HTTP Error \" + str(results.get('status', 'no status')) + \" in query\")\n",
    "    else:\n",
    "        results = results['entries']\n",
    "    for result in results:\n",
    "        modify_query_result(result)\n",
    "        if prune:\n",
    "            prune_query_result(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def modify_query_result(result):\n",
    "    result['pdf_url'] = None\n",
    "    for link in result['links']:\n",
    "        if 'title' in link and link['title'] == 'pdf':\n",
    "            result['pdf_url'] = link['href']\n",
    "    result['affiliation'] = result.pop('arxiv_affiliation', 'None')\n",
    "    result['arxiv_url'] = result.pop('link')\n",
    "    result['title'] = result['title'].rstrip('\\n')\n",
    "    result['summary'] = result['summary'].rstrip('\\n')\n",
    "    result['authors'] = [d['name'] for d in result['authors']]\n",
    "    if 'arxiv_comment' in result:\n",
    "        result['arxiv_comment'] = result['arxiv_comment'].rstrip('\\n')\n",
    "    else:\n",
    "        result['arxiv_comment'] = None\n",
    "    if 'arxiv_journal_ref' in result:\n",
    "        result['journal_reference'] = result.pop('arxiv_journal_ref')\n",
    "    else:\n",
    "        result['journal_reference'] = None\n",
    "    if 'arxiv_doi' in result:\n",
    "        result['doi'] = result.pop('arxiv_doi')\n",
    "    else:\n",
    "        result['doi'] = None\n",
    "        \n",
    "def prune_query_result(result):\n",
    "    prune_keys = ['updated_parsed',\n",
    "                              'arxiv_primary_category',\n",
    "                              'summary_detail',\n",
    "                              'author',\n",
    "                              'author_detail',\n",
    "                              'links',\n",
    "                              'guidislink',\n",
    "                              'title_detail',\n",
    "                              'tags',\n",
    "                              'id']\n",
    "    for key in prune_keys:\n",
    "        try:\n",
    "            del result[key]\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "def select_recent_papers(from_papers_list, days, debug):\n",
    "    today = datetime.datetime.today()\n",
    "    utc_today = today - relativedelta(hours=9)\n",
    "\n",
    "    from_when = utc_today - relativedelta(days=days)\n",
    "    to_when = utc_today\n",
    "\n",
    "    if debug:\n",
    "        print('JST: ', today)\n",
    "        print('UTC_from: ', from_when)\n",
    "        print('UTC_to  : ', to_when)\n",
    "        print()\n",
    "        print('recent papers\\' timestamps are like below:')\n",
    "        for paper in from_papers_list:\n",
    "            print(paper['published'])\n",
    "\n",
    "    return list(filter(lambda x: condition_to_select_papers(x, from_when, to_when), from_papers_list))\n",
    "\n",
    "def condition_to_select_papers(paper, from_when, to_when):\n",
    "    return from_when <= datetime.datetime(*paper['published_parsed'][:6]) < to_when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for keyword in keywords:\n",
    "    arxiv_lists = make_list(root_url, keyword, prune, start, max_results, sort_by, sort_order, days, debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'updated': '2020-06-04T16:43:55Z',\n",
       " 'published': '2020-06-04T16:43:55Z',\n",
       " 'published_parsed': time.struct_time(tm_year=2020, tm_mon=6, tm_mday=4, tm_hour=16, tm_min=43, tm_sec=55, tm_wday=3, tm_yday=156, tm_isdst=0),\n",
       " 'title': 'Differentiable Linear Bandit Algorithm',\n",
       " 'summary': 'Upper Confidence Bound (UCB) is arguably the most commonly used method for\\nlinear multi-arm bandit problems. While conceptually and computationally\\nsimple, this method highly relies on the confidence bounds, failing to strike\\nthe optimal exploration-exploitation if these bounds are not properly set. In\\nthe literature, confidence bounds are typically derived from concentration\\ninequalities based on assumptions on the reward distribution, e.g.,\\nsub-Gaussianity. The validity of these assumptions however is unknown in\\npractice. In this work, we aim at learning the confidence bound in a\\ndata-driven fashion, making it adaptive to the actual problem structure.\\nSpecifically, noting that existing UCB-typed algorithms are not differentiable\\nwith respect to confidence bound, we first propose a novel differentiable\\nlinear bandit algorithm. Then, we introduce a gradient estimator, which allows\\nthe confidence bound to be learned via gradient ascent. Theoretically, we show\\nthat the proposed algorithm achieves a\\n$\\\\tilde{\\\\mathcal{O}}(\\\\hat{\\\\beta}\\\\sqrt{dT})$ upper bound of $T$-round regret,\\nwhere $d$ is the dimension of arm features and $\\\\hat{\\\\beta}$ is the learned\\nsize of confidence bound. Empirical results show that $\\\\hat{\\\\beta}$ is\\nsignificantly smaller than its theoretical upper bound and proposed algorithms\\noutperforms baseline ones on both simulated and real-world datasets.',\n",
       " 'authors': ['Kaige Yang', 'Laura Toni'],\n",
       " 'arxiv_comment': '16 pages',\n",
       " 'pdf_url': 'http://arxiv.org/pdf/2006.03000v1',\n",
       " 'affiliation': 'None',\n",
       " 'arxiv_url': 'http://arxiv.org/abs/2006.03000v1',\n",
       " 'journal_reference': None,\n",
       " 'doi': None}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(arxiv_lists))\n",
    "arxiv_lists[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate arxiv articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(result):\n",
    "    translator = Translator()\n",
    "    title = []\n",
    "    summary = []\n",
    "    title_and_summary = []\n",
    "    result_title = []\n",
    "    result_summary = []\n",
    "    title = list(map(lambda x: x['title'].replace('\\n', ' '), result))\n",
    "    summary = list(map(lambda x: x['summary'].replace('\\n', ' '), result))\n",
    "    published = list(map(lambda x: x['published'], result))\n",
    "    arxiv_url = list(map(lambda x: x['arxiv_url'], result))\n",
    "    pdf_url = list(map(lambda x: x['pdf_url'], result))\n",
    "\n",
    "    for i in zip(title, summary):\n",
    "        title_and_summary.append(i)\n",
    "\n",
    "    for i in range(len(title_and_summary)):\n",
    "        tas = title_and_summary[i]\n",
    "        text_title =  tas[0]\n",
    "        text_summary = tas[1]\n",
    "        translate_title = translator.translate(text_title, dest='ja', src='en')\n",
    "        translate_summary = translator.translate(text_summary, dest='ja', src='en')\n",
    "        result_title.append(translate_title.text)\n",
    "        result_summary.append(translate_summary.text)\n",
    "        print(\"No.{}, title:{}\".format(i, translate_title.text))\n",
    "    result_title = list(map(lambda x: x.replace('\\n\\xa0\\xa0', ':'), result_title))\n",
    "    result_summary = list(map(lambda x: x.replace('\\n\\xa0\\xa0', ':'), result_summary))\n",
    "\n",
    "    return title, summary, published, arxiv_url, pdf_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "title, summary, published, arxiv_url, pdf_url = translate(arxiv_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = arxiv_lists\n",
    "# translator = Translator()\n",
    "# title = []\n",
    "# summary = []\n",
    "# title_and_summary = []\n",
    "# result_title = []\n",
    "# result_summary = []\n",
    "# published = list(map(lambda x: x['published'], result))\n",
    "# url = list(map(lambda x: x['pdf_url'], result))\n",
    "# title = list(map(lambda x: x['title'].replace('\\n', ' '), result))\n",
    "# summary = list(map(lambda x: x['summary'].replace('\\n', ' '), result))\n",
    "\n",
    "# for i in zip(title, summary):\n",
    "#     title_and_summary.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# i = 10\n",
    "# tas = title_and_summary[i]\n",
    "# text_title =  tas[0]\n",
    "# text_summary = tas[1]\n",
    "# translate_title = translator.translate(text_title, dest='ja', src='en')\n",
    "# translate_summary = translator.translate(text_summary, dest='ja', src='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "translated_arxiv_pd = pd.DataFrame({\n",
    "    'title': title, \n",
    "    'summary': summary,\n",
    "    'published': published,\n",
    "    'arxiv_url': arxiv_url,\n",
    "    'pdf_url': pdf_url\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_arxiv_pd.to_csv('translated_arxiv_v0.csv', index=False, encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the data with label, and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('translated_arxiv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
